# MIMIR-AI Configuration
# Copy this file to .env and adjust the values

# ============================================
# Model Configuration
# ============================================
MODEL_NAME=llama-3.2-3b-instruct-q8_0.gguf
MODELS_DIR=./models

# Embedding Model Configuration (local embeddings via llama-server)
EMBEDDING_MODEL_NAME=nomic-embed-text-v1.5.Q5_K_M.gguf
# Leave empty to auto-detect embedding model from MODELS_DIR

# ============================================
# Engine Configuration (llama-server)
# ============================================
ENGINE_PORT=8080
EMBEDDING_ENGINE_PORT=8081
BIN_DIR=./bin
STORAGE_DIR=./storage

# ============================================
# Generation Parameters
# ============================================
TEMPERATURE=0.7
TOP_P=0.9
TOP_K=40
MAX_TOKENS=2048

# ============================================
# Memory Configuration
# ============================================
MAX_HISTORY_LENGTH=10
CONTEXT_WINDOW_MESSAGES=5
MEMORY_EXPIRY=3600


# ============================================
# RAG Memory Manager Configuration
# ============================================
# Enable or disable RAG system (true/false)
RAG_ENABLED=true

# Use local embeddings via llama-server (true/false)
# If true, uses local GGUF embedding model via llama-server
# If false, uses sentence-transformers (requires internet)
USE_LOCAL_EMBEDDINGS=true

# Embedding model for RAG (fallback if USE_LOCAL_EMBEDDINGS=false)
# sentence-transformers model from HuggingFace (only used as fallback)
# Default: nomic-ai/nomic-embed-text-v1.5
# Other options: all-MiniLM-L6-v2, sentence-transformers/all-mpnet-base-v2
RAG_EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5

# Size of text chunks for document processing
RAG_CHUNK_SIZE=512

# Overlap between consecutive chunks
RAG_CHUNK_OVERLAP=50

# Number of documents to retrieve in similarity search
RAG_TOP_K=4

# Name of the ChromaDB collection for RAG memory
RAG_COLLECTION_NAME=mimir_memory
